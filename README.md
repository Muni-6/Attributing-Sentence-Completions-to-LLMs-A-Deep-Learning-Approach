
# CSE 584 Midterm Project
## Attributing-Sentence-Completions-to-LLMs-A-Deep-Learning-Approach

This project aims to classify the output generated by various Large Language Models (LLMs) using a deep learning classifier. The classifier is trained to identify which LLM produced a given pair of text fragments: a truncated text (xi) and its completion (xj). Two distinct models were used for this classification task:

1. **DistilBERT Model**
2. **Neural Network with Contextual Document Embeddings (CDE)**

### Project Overview

The main objective of this project was to attribute text completions to their respective LLMs. The dataset used consisted of pairs of truncated sentences and their corresponding LLM-generated completions, collected from multiple models, including:

- **LLaMA 3.2 (1B)**
- **Qwen 2.5 (3B)**
- **Phi-3.5 (3B)**
- **Granite-Code**
- **CodeGemma**
- **Mistral-OpenOrca**

The dataset was curated and analyzed to understand text generation patterns across different models, and a deep learning classifier was developed to predict which LLM was responsible for generating the output.

#### 1. DistilBERT Model

- **Embedding Generation**: We used Hugging Face's `distilbert-base-uncased` model for this task, as it is a smaller and more efficient version of BERT.
- **Tokenization**: Each sentence fragment pair (xi and xj) was tokenized independently, converted into embeddings using the [CLS] token.
- **Classification Layer**: After embedding extraction, the [CLS] token from both xi and xj were concatenated and passed through a dense neural network for classification.
- **Training and Evaluation**: The model was trained with cross-entropy loss and Adam optimizer, monitoring accuracy and loss per epoch.

#### 2. Neural Network with CDE

- **Two-stage Embedding Process**: This model uses a pre-trained transformer to generate embeddings from both xi and xj. The embeddings were then refined in a second stage, producing contextual embeddings for classification.
- **Neural Network Classifier**: After extracting embeddings, they were concatenated and passed through a fully connected neural network with batch normalization, dropout layers, and ReLU activations.
- **Training and Hyperparameter Tuning**: Experiments were conducted with various learning rates, batch sizes, and epochs to optimize the classifier.

### Dataset

We curated a dataset of **30,000 pairs** of truncated sentences and their corresponding completions generated by different LLMs using Ollama, a platform that allows models to be executed locally. The dataset was split into:

- **Training Set**: 80% (24,000 pairs)
- **Validation Set**: 10% (3,000 pairs)
- **Test Set**: 10% (3,000 pairs)

The dataset included sentences from :
**Tatoeba Project**: Diverse and simple sentence structures ideal for sentence generation.

### Results

- **DistilBERT**: Achieved an accuracy of **72.9%** on the test set, with good generalization and moderate overfitting observed through training loss curves.
- **Neural Network with CDE**: Achieved an accuracy of **57%** with slightly lower performance compared to DistilBERT, but showed more stability in precision and recall for certain LLMs.

Both models demonstrated the ability to distinguish between LLMs based on their text generation patterns, but future work may improve on these models with additional architectural improvements or ensemble methods.

### How to Run

#### Installation Requirements

To run the code, install the following dependencies:

- Python 3.7+
- PyTorch
- Hugging Face Transformers
- TensorFlow/Keras (for CDE)
- scikit-learn
- pandas

#### Running the Code

1. **Data Preparation**: Prepare your dataset in a CSV file with columns: `xi`, `xj`, and `model`. This can be done by running the provided `generate_xj.py` script.
2. **Model Training**: Run the training script to train either the DistilBERT or Neural Network model on your dataset. Use the `.ipynb` notebook for running training and evaluating model performance.


### License

This project is licensed under the MIT License.
